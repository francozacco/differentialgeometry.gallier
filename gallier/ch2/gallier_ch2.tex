\documentclass[11pt]{article}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{adjustbox}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{tikz-cd}
\usepackage[mathscr]{euscript}


\title{\textbf{
    Solutions to selected problems on Differential Geometry and Lie Groups
    - Gallier.
}}
\author{Franco Zacco}
\date{}

\addtolength{\topmargin}{-3cm}
\addtolength{\textheight}{3cm}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\diam}{\text{diam}}
\newcommand{\cl}{\text{cl}}
\newcommand{\bdry}{\text{bdry}}
\newcommand{\inter}{\text{Int}}
\newcommand{\ext}{\text{Ext}}
\newcommand{\Pow}{\mathcal{P}}
\newcommand{\Topo}{\mathcal{T}}
\newcommand{\Or}{\text{ or }}
\newcommand{\setmin}{\setminus}
\newcommand{\tr}{\text{tr}}
\newcommand{\End}{\text{End}}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\dom}{\mathscrsfs{D}}
\newcommand{\range}{\mathscrsfs{R}}
\newcommand{\nullsp}{\mathscrsfs{N}}
\newcommand{\inprd}[1]{\langle{#1}\rangle}



\DeclareSymbolFontAlphabet{\mathscrsfs}{rsfs}
\theoremstyle{definition}
\newtheorem*{solution*}{Solution}

\begin{document}
\maketitle
\thispagestyle{empty}

Solutions to selected problems on Differential Geometry and Lie Groups from
different books, but mainly from Gallier.
\section*{Chapter 2 - The Matrix Exponential: Some Matrix and Lie Groups}
\subsection*{2.1 The Exponential Map}
\begin{proof}{\textbf{Problem 2.1.}}
\begin{itemize}
    \item [(a)] Let the following symmetric matrices
    \begin{align*}
        A = \begin{pmatrix}
            2 & 0\\
            0 & 1/2
        \end{pmatrix}
        \quad
        B = \begin{pmatrix}
            1 & 2\\
            2 & 1
        \end{pmatrix}
    \end{align*}
    Then $AB$ is given by
    \begin{align*}
        AB &= \begin{pmatrix}
            2 & 0\\
            0 & 1/2
        \end{pmatrix}
        \cdot
        \begin{pmatrix}
            1 & 2\\
            2 & 1
        \end{pmatrix}
        = \begin{pmatrix}
            2 + 0 & 4 + 0\\
            0 + 1 & 0 + 1/2
        \end{pmatrix}
        = \begin{pmatrix}
            2 & 4\\
            1 & 1/2
        \end{pmatrix}
    \end{align*}
    Which is non-symmetric.

    \item [(b)] Let the following skew symmetric matrices
    \begin{align*}
        A = \pi\begin{pmatrix}
            0 & 0 & 0\\
            0 & 0 & -1\\
            0 & 1 & 0
        \end{pmatrix}
        \quad
        B = \pi\begin{pmatrix}
            0 & 0 & 1\\
            0 & 0 & 0\\
            -1 & 0 & 0
        \end{pmatrix}
    \end{align*}
    We can work out a explicit formula for $e^A$ and $e^B$ by using
    Rodrigues formula. For $A$ we see that $A$ has the required form
    \begin{align*}
        A = \begin{pmatrix}
            0 & -c & b\\
            c & 0 & -a\\
            -b & a & 0
        \end{pmatrix}
    \end{align*}
    taking $a = \pi$ and $b = c = 0$ we have that $\theta = \pi$.
    Hence by Rodrigues formula we have that
    \begin{align*}
        e^A &= \begin{pmatrix}
            1 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
        + 0 \cdot \begin{pmatrix}
            0 & 0 & 0\\
            0 & 0 & -1\\
            0 & 1 & 0
        \end{pmatrix}
        + \frac{2}{\pi^2} \cdot \begin{pmatrix}
            0 & 0 & 0\\
            0 & 0 & -1\\
            0 & 1 & 0
        \end{pmatrix}^2\\
        &= \begin{pmatrix}
            1 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
        + \frac{2}{\pi^2} \cdot \begin{pmatrix}
            0 & 0 & 0\\
            0 & -1 & 0\\
            0 & 0 & -1
        \end{pmatrix}\\
        &= \begin{pmatrix}
            1 & 0 & 0\\
            0 & \frac{\pi^2 - 2}{\pi^2} & 0\\
            0 & 0 & \frac{\pi^2 - 2}{\pi^2}
        \end{pmatrix}  
    \end{align*}
    In the case of $B$ we have that $b = \pi$ and $a = c = 0$, hence
    $\theta = \pi$ then
    \begin{align*}
        e^B &= \begin{pmatrix}
            1 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
        + 0 \cdot \begin{pmatrix}
            0 & 0 & 1\\
            0 & 0 & 0\\
            -1 & 0 & 0
        \end{pmatrix}
        + \frac{2}{\pi^2} \cdot \begin{pmatrix}
            0 & 0 & 1\\
            0 & 0 & 0\\
            -1 & 0 & 0
        \end{pmatrix}^2\\
        &= \begin{pmatrix}
            1 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
        + \frac{2}{\pi^2} \cdot \begin{pmatrix}
            -1 & 0 & 0\\
            0 & 0 & 0\\
            0 & 0 & -1
        \end{pmatrix}\\
        &= \begin{pmatrix}
            \frac{\pi^2 - 2}{\pi^2} & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & \frac{\pi^2 - 2}{\pi^2}
        \end{pmatrix}        
    \end{align*}
    Now, we compute $e^Ae^B$ as follows
    \begin{align*}
        e^Ae^B &= \begin{pmatrix}
            1 & 0 & 0\\
            0 & \frac{\pi^2 - 2}{\pi^2} & 0\\
            0 & 0 & \frac{\pi^2 - 2}{\pi^2}
        \end{pmatrix}\begin{pmatrix}
            \frac{\pi^2 - 2}{\pi^2} & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & \frac{\pi^2 - 2}{\pi^2}
        \end{pmatrix}\\
        &= \begin{pmatrix}
            \frac{\pi^2 - 2}{\pi^2} & 0 & 0\\
            0 & \frac{\pi^2 - 2}{\pi^2} & 0\\
            0 & 0 & \big(\frac{\pi^2 - 2}{\pi^2}\big)^2
        \end{pmatrix}
    \end{align*}
    Finally, we compute $e^{A + B}$ using Rodrigues formula as well since the
    matrix $A + B$ is skew symmetric as we want, where $a = b = \pi$ and $c = 0$
    then we have that $\theta = \sqrt{2}\pi$ and hence
    \begin{align*}
        e^{A + B} &= \begin{pmatrix}
            1 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
        + \frac{\sin(\sqrt{2}\pi)}{\sqrt{2}}\begin{pmatrix}
            0 & 0 & 1\\
            0 & 0 & -1\\
            -1 & 1 & 0
        \end{pmatrix}
        + \frac{1 - \cos(\sqrt{2}\pi)}{2}\begin{pmatrix}
            0 & 0 & 1\\
            0 & 0 & -1\\
            -1 & 1 & 0
        \end{pmatrix}^2\\
        e^{A + B} &= \begin{pmatrix}
            1 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
        + \frac{\sin(\sqrt{2}\pi)}{\sqrt{2}}\begin{pmatrix}
            0 & 0 & 1\\
            0 & 0 & -1\\
            -1 & 1 & 0
        \end{pmatrix}
        + \frac{1 - \cos(\sqrt{2}\pi)}{2}\begin{pmatrix}
            -1 & 1 & 0\\
            1 & -1 & 0\\
            0 & 0 & -2
        \end{pmatrix}\\
        e^{A + B} &= \begin{pmatrix}
            1-\frac{1 - \cos(\sqrt{2}\pi)}{2} &
            \frac{1 - \cos(\sqrt{2}\pi)}{2} &
            \frac{\sin(\sqrt{2}\pi)}{\sqrt{2}}\\
            \frac{1 - \cos(\sqrt{2}\pi)}{2} &
            1 -\frac{1 - \cos(\sqrt{2}\pi)}{2} &
            -\frac{\sin(\sqrt{2}\pi)}{\sqrt{2}}\\
            -\frac{\sin(\sqrt{2}\pi)}{\sqrt{2}} &
            \frac{\sin(\sqrt{2}\pi)}{\sqrt{2}} &
            \cos(\sqrt{2}\pi)
        \end{pmatrix}
    \end{align*}
    Therefore we see that $e^Ae^B \neq e^{A + B}$.

    \item [(c)] Let the following matrices
    \begin{align*}
        A = \begin{pmatrix} 2\pi i & 0\\ 0 & 0 \end{pmatrix}
        \quad
        B = \begin{pmatrix} 2\pi i & 1\\ 0 & 0 \end{pmatrix}
    \end{align*}
    We see that 
    \begin{align*}
        AB &= \begin{pmatrix} 2\pi i & 0\\ 0 & 0 \end{pmatrix}
        \cdot \begin{pmatrix} 2\pi i & 1\\ 0 & 0 \end{pmatrix}
        = \begin{pmatrix} -4\pi^2 & 2\pi i\\ 0 & 0 \end{pmatrix}\\
        BA &= \begin{pmatrix} 2\pi i & 1\\ 0 & 0 \end{pmatrix}
        \cdot \begin{pmatrix} 2\pi i & 0\\ 0 & 0 \end{pmatrix}
        = \begin{pmatrix} -4\pi^2 &  0\\ 0 & 0 \end{pmatrix}
    \end{align*}
    Hence $AB \neq BA$.

    Now, we need to compute $e^A$ so first we compute $A^2, A^3, ...$ as follows
    \begin{align*}
        A^2 = \begin{pmatrix} 2\pi i & 0\\ 0 & 0 \end{pmatrix}^2
        = \begin{pmatrix} -4\pi^2 &  0\\ 0 & 0 \end{pmatrix}\\
        A^3 = \begin{pmatrix} 2\pi i & 0\\ 0 & 0 \end{pmatrix}^3
        = \begin{pmatrix} -8i\pi^3 &  0\\ 0 & 0 \end{pmatrix}
    \end{align*}
    So we see that
    \begin{align*}
        A^n = (2\pi i)^{n}\begin{pmatrix} 1 & 0\\ 0 & 0 \end{pmatrix}
        = (2\pi i)^{n}J_A
    \end{align*}
    Where
    $$J_A = \begin{pmatrix} 1 & 0\\ 0 & 0 \end{pmatrix}$$
    Hence
    \begin{align*}
        e^A &= I_2 + \bigg((2\pi i)J_A + \frac{(2\pi i)^2}{2!}J_A
        + \frac{(2\pi i)^3}{3!}J_A + \frac{(2\pi i)^4}{4!}J_A + ...\bigg)\\
        &= I_2 + \sum_{n=1}^\infty \frac{(2\pi i)^n}{n!}J_A\\
        &= I_2
    \end{align*}
    To compute $e^B$ we follow the same path, we see that 
    \begin{align*}
        B^2 &= \begin{pmatrix} 2\pi i & 1\\ 0 & 0 \end{pmatrix}^2
        = \begin{pmatrix} -4\pi^2 &  2\pi i\\ 0 & 0 \end{pmatrix}\\
        B^3 &= \begin{pmatrix} 2\pi i & 1\\ 0 & 0 \end{pmatrix}^3
        = \begin{pmatrix} -8i\pi^3 & -4\pi^2\\ 0 & 0 \end{pmatrix}
    \end{align*}
    So
    \begin{align*}
        B^n = (2\pi i)^{n}\begin{pmatrix} 1 & 1/(2\pi i)\\ 0 & 0 \end{pmatrix}
        = (2\pi i)^{n}J_B
    \end{align*}
    Where
    $$J_B = \begin{pmatrix} 1 & 1/(2\pi i)\\ 0 & 0 \end{pmatrix}$$
    Hence
    \begin{align*}
        e^B &= I_2 + \bigg((2\pi i)J_B + \frac{(2\pi i)^2}{2!}J_B 
        + \frac{(2\pi i)^3}{3!}J_B + \frac{(2\pi i)^4}{4!}J_B +...\bigg)\\
        &= I_2 + \sum_{n=1}^\infty \frac{(2\pi i)^n}{n!}J_B\\
        &= I_2
    \end{align*}
    Therefore 
    \begin{align*}
        e^Ae^B &= I_2 \cdot I_2
        = \begin{pmatrix} 1 & 0\\ 0 & 1 \end{pmatrix}
    \end{align*}
    But also we have that 
    \begin{align*}
        A + B &= \begin{pmatrix} 4\pi i & 1\\ 0 & 0 \end{pmatrix}\\
        (A + B)^2 &= \begin{pmatrix} 4\pi i & 1\\ 0 & 0 \end{pmatrix}^2
        = \begin{pmatrix} -16\pi^2 & 4\pi i\\ 0 & 0 \end{pmatrix}\\
        (A + B)^3 &= \begin{pmatrix} 4\pi i & 1\\ 0 & 0 \end{pmatrix}^3
        = \begin{pmatrix} -64\pi^3 i & -16\pi^2\\ 0 & 0 \end{pmatrix}
    \end{align*}
    So we see that
    \begin{align*}
        (A + B)^n = (4\pi i)^{n}\begin{pmatrix} 1 & 1/(4\pi i)\\ 0 & 0\end{pmatrix}
        = (4\pi i)^{n}J_{A + B}
    \end{align*}
    Where
    $$J_{A+B} = \begin{pmatrix} 1 & 1/(4\pi i)\\ 0 & 0 \end{pmatrix}$$
    Hence 
    \begin{align*}
        e^{A + B} &= I_2 + \bigg((4\pi i) J_{A+B}+ \frac{(4\pi i)^2}{2!}J_{A+B}
        + \frac{(4\pi i)^3}{3!}J_{A+B} + \frac{(4\pi i)^4}{4!}J_{A+B} +...\bigg)\\
        &= I_2 + \sum_{n=1}^\infty \frac{(4\pi i)^n}{n!}J_{A+B}\\
        &= I_2
    \end{align*}
    Therefore we obtain that $e^Ae^B = e^{A+B}$.
\end{itemize}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hubbard: Problem 1.5.10}}
\begin{itemize}
    \item[a.] Let 
    \begin{align*}
        e^A = \sum_{k=0}^\infty \frac{A^k}{k!} = I + A + \frac{1}{2}A^2 +
        \frac{1}{3!}A^3 + ...
    \end{align*}
    Because of Proposition 2.1 from Gallier we know that the series is
    absolutely convergent this implies that the original series is convergent
    as well.

    Also, it is shown that the series $|e^A|$ is bounded by the following
    series 
    \begin{align*}
        e^{n\mu} = \sum_{k=0}^\infty \frac{(n\mu)^k}{k!}
    \end{align*}
    where $\mu = \max\{|a_{ij}|: 1 \leq i,j \leq n\}$ and $A = (a_{ij})$.

    \item[b.]
    \begin{itemize}
        \item [i.] Let
        \begin{align*}
            A = \begin{bmatrix} a & 0 \\ 0 & b \end{bmatrix}
        \end{align*}
        Then 
        \begin{align*}
            &A^2 = \begin{bmatrix} a & 0 \\ 0 & b \end{bmatrix}^2
            = \begin{bmatrix} a^2 & 0 \\ 0 & b^2 \end{bmatrix}\\
            &A^3 = \begin{bmatrix} a & 0 \\ 0 & b \end{bmatrix}^3
            = \begin{bmatrix} a^3 & 0 \\ 0 & b^3 \end{bmatrix}\\
            &\dots\\
            &A^k = \begin{bmatrix} a & 0 \\ 0 & b \end{bmatrix}^k
            = \begin{bmatrix} a^k & 0 \\ 0 & b^k \end{bmatrix}
        \end{align*}
        So we have that
        \begin{align*}
            e^A = \sum_{k=0}^\infty \frac{1}{k!}
            \begin{bmatrix} a^k & 0 \\ 0 & b^k \end{bmatrix}
            =\begin{bmatrix} \sum_{k=0}^\infty \frac{a^k}{k!} & 0 \\
            0 & \sum_{k=0}^\infty \frac{b^k}{k!} \end{bmatrix}
            =\begin{bmatrix} e^a & 0 \\ 0 & e^b \end{bmatrix}
        \end{align*}
        \item [ii.] Let
        \begin{align*}
            A = \begin{bmatrix} 0 & a \\ 0 & 0 \end{bmatrix}
        \end{align*}
        In this case, the matrix has a null trace so according to
        Gallier we can compute $e^A$ as follows
        \begin{align*}
            e^A = I_2 + A = \begin{bmatrix} 1 & a \\ 0 & 1 \end{bmatrix}
        \end{align*}
        \item [iii.] Let 
        \begin{align*}
            A = \begin{bmatrix} 0 & a \\ -a & 0 \end{bmatrix}
            = -a\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
        \end{align*}
        Then from Gallier we know that
        \begin{align*}
            e^A = \begin{bmatrix} \cos(-a) & -\sin(-a) \\
            \sin(-a) & \cos(-a) \end{bmatrix}
            = \begin{bmatrix} \cos a & \sin a \\
            -\sin a & \cos a \end{bmatrix}
        \end{align*}
    \end{itemize}

    \item[c.]
    \begin{itemize}
        \item [1.] Let
        \begin{align*}
            A = \begin{bmatrix} 0 & a\\ 0 & 0 \end{bmatrix}
            \quad\quad
            B = \begin{bmatrix} 0 & 0\\ b & 0 \end{bmatrix}            
        \end{align*}
        Such that $a, b > 0$. Then
        \begin{align*}
            e^A = \begin{bmatrix} 1 & a\\ 0 & 1 \end{bmatrix}
            \quad\quad
            e^B = \begin{bmatrix} 1 & 0\\ b & 1 \end{bmatrix}
        \end{align*}
        Because they are null trace matrices. So
        \begin{align*}
            e^Ae^B = \begin{bmatrix} 1 & a\\ 0 & 1 \end{bmatrix}
            \cdot \begin{bmatrix} 1 & 0\\ b & 1 \end{bmatrix}
            = \begin{bmatrix} 1 + ab & a\\ b & 1 \end{bmatrix}
        \end{align*}
        But 
        \begin{align*}
            A+B = \begin{bmatrix} 0 & a\\ b & 0 \end{bmatrix}
        \end{align*}
        Hence by what we saw on Gallier we get that
        \begin{align*}
            e^{A+B} &= \cosh(\sqrt{ab}) I_2 + \frac{\sinh(\sqrt{ab})}{\sqrt{ab}}(A + B)\\
            &= \begin{bmatrix} \cosh(\sqrt{ab}) & 0\\ 0 & \cosh(\sqrt{ab}) \end{bmatrix}
            + \begin{bmatrix} 0 & \frac{a\sinh(\sqrt{ab})}{\sqrt{ab}}\\
            \frac{b\sinh(\sqrt{ab})}{\sqrt{ab}} & 0 \end{bmatrix}\\
            &= \begin{bmatrix} \cosh(\sqrt{ab}) & \frac{a\sinh(\sqrt{ab})}{\sqrt{ab}}\\
            \frac{b\sinh(\sqrt{ab})}{\sqrt{ab}} & \cosh(\sqrt{ab}) \end{bmatrix}
        \end{align*}
        Therefore $e^{A + B} \neq e^Ae^B$.
        \item [2.] We proved this on Gallier Proposition 2.5. 
        \item [3.] Let $A$ be an $n \times n$ matrix. Using the sub-problem
        number 2 and given that $A$ commutes with itself we have that
        \begin{align*}
            e^{A + A} &= e^Ae^A
        \end{align*}
        i.e.
        \begin{align*}
            e^{2A} &= (e^A)^2
        \end{align*}
    \end{itemize}
\end{itemize}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 2.6.3}}
    Let $A(t)$ and $B(t)$ be two smooth matrix-valued functions then by
    definition we have that
    \begin{align*}
        \frac{d}{dt}[A(t)B(t)] &= \lim_{h\to 0}\frac{A(t+h)B(t+h) - A(t)B(t)}{h}\\
            &= \lim_{h\to 0}\frac{A(t+h)B(t+h) - A(t)B(t) + A(t)B(t+h) - A(t)B(t+h)}{h}\\
            &= \lim_{h\to 0}\frac{B(t+h)(A(t+h) - A(t)) + A(t)(B(t+h) - B(t))}{h}\\
            &= \lim_{h\to 0}\frac{B(t+h)(A(t+h) - A(t))}{h}
            + \lim_{h\to 0}\frac{A(t)(B(t+h) - B(t))}{h}\\
            &= B(t)\frac{dA}{dt} + A(t)\frac{dB}{dt}
    \end{align*}
    Now we want to prove that $A(t)B(t)$ is smooth, we see that the element
    $jk$ of $A(t)B(t)$ is given by
    \begin{align*}
        (A(t)B(t))_{jk} &= \sum_{i=0}^n A_{ji}(t)B_{ik}(t)
    \end{align*}
    Given that $A_{ji}(t)$ and $B_{ik}(t)$ are smooth functions from $\R$ to
    $\R$ then the product $A_{ji}(t)B_{ik}(t)$ is smooth and the sum of smooth
    functions is also smooth. Hence $(A(t)B(t))_{jk}$ is smooth and therefore
    $A(t)B(t)$ is smooth. 
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 2.6.4}}
    Let $A$ be an $n \times n$ complex matrix. If $A$ is diagonalizable then
    we have that $\lim_{m \to \infty} A = A$ and hence we are done.

    Suppose $A$ is not diagonalizable, then we know it's similar to an upper
    triangular matrix $U$ i.e.
    \begin{align*}
        A = P U P^{-1}
    \end{align*}
    Then $U$ has the form
    \begin{align*}
        U = \begin{bmatrix}
            u_{11} & u_{12} & \dots & u_{1n}\\
            0 & u_{22} & \dots & u_{2n}\\
            \vdots & \vdots & \dots & \vdots\\
            0 & 0 & \dots & u_{nn}\\
        \end{bmatrix}
    \end{align*}
    Where $u_{11}, u_{22}, \dots, u_{nn}$ are the eigenvalues of $U$. Given
    that $A$ is not diagonalizable then $U$ is not diagonalizable then 
    one or more eigenvalues are equal. Suppose we modify the eigenvalues
    of $U$ as follows, we take $u_{22}$ if it's equal to $u_{11}$ we modify it
    as $u_{22} - 1/m$ then we take $u_{33}$ if it's equal to $u_{11}$ or
    $u_{22}$ we modify it as $u_{33} - 2/m$ and we continue this process until
    $u_{nn}$. In the worst case, where all the eigenvalues are equal we
    end up with a matrix $U_m$ as follows
    \begin{align*}
        U_m = \begin{bmatrix}
            u_{11} & u_{12} & \dots & u_{1n}\\
            0 & u_{22} - \frac{1}{m} & \dots & u_{2n}\\
            \vdots & \vdots & \dots & \vdots\\
            0 & 0 & \dots & u_{nn} - \frac{n-1}{m}\\
        \end{bmatrix}
    \end{align*}
    We see that $U_m$ for any $m \in \N$ will be diagonalizable but also we see
    that $\lim_{m \to \infty} U_m = U$ hence
    \begin{align*}
        \lim_{m \to \infty} P U_m P^{-1} = P U P^{-1} = A
    \end{align*}
    Therefore $A$ is the limit of a sequence of diagonalizable matrices.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 2.6.5}}
    Let $a \neq d$ then
    \begin{align*}
        \begin{pmatrix} a & b\\ 0 & d \end{pmatrix}^2
        &= \begin{pmatrix} a & b\\ 0 & d \end{pmatrix}
        \cdot \begin{pmatrix} a & b\\ 0 & d \end{pmatrix}\\
        &= \begin{pmatrix} a^2 & ab + bd\\ 0 & d^2 \end{pmatrix}\\
        &= \begin{pmatrix} a^2 & b(a + d)\frac{a - d}{a - d}\\ 0 & d^2 \end{pmatrix}\\
        &= \begin{pmatrix} a^2 & b\frac{a^2 - d^2}{a - d}\\ 0 & d^2 \end{pmatrix}
    \end{align*}
    Also, we see that
    \begin{align*}
        \begin{pmatrix} a & b\\ 0 & d \end{pmatrix}^3
        &= \begin{pmatrix} a & b\\ 0 & d \end{pmatrix}^2
        \cdot \begin{pmatrix} a & b\\ 0 & d \end{pmatrix}\\
        &= \begin{pmatrix} a^2 & b\frac{a^2 - d^2}{a - d}\\ 0 & d^2 \end{pmatrix}
        \cdot \begin{pmatrix} a & b\\ 0 & d \end{pmatrix}\\
        &= \begin{pmatrix} a^3 & a^2b + bd\frac{a^2 - d^2}{a - d}\\ 0 & d^3\end{pmatrix}\\
        &= \begin{pmatrix}
            a^3 & b\frac{a^2(a - d) + da^2 - d^3}{a - d}\\ 0 & d^3
        \end{pmatrix}\\
        &= \begin{pmatrix}
            a^3 & b\frac{a^3 - d^3}{a - d}\\ 0 & d^3
        \end{pmatrix}
    \end{align*}
    We can continue this process $m$ times so we can write that
    \begin{align*}
        \begin{pmatrix} a & b\\ 0 & d \end{pmatrix}^m
        &= \begin{pmatrix}
            a^m & b\frac{a^m - d^m}{a - d}\\ 0 & d^m
        \end{pmatrix}
    \end{align*}
    Now we compute $\exp\begin{pmatrix} a & b\\ 0 & d\end{pmatrix}$ as follows
    \begin{align*}
        \exp\begin{pmatrix} a & b\\ 0 & d\end{pmatrix}
        &= \sum_{k = 0}^\infty\frac{1}{k!}
        \begin{pmatrix} a & b\\ 0 & d \end{pmatrix}^k\\
        &= \sum_{k=0}^\infty\frac{1}{k!}
        \begin{pmatrix} a^k & b\frac{a^k - d^k}{a - d}\\ 0 & d^k\end{pmatrix}\\
        &= \begin{pmatrix}
            \sum_{k=0}^\infty\frac{1}{k!} a^k &
            \frac{b}{a - d}\sum_{k=0}^\infty\frac{1}{k!}(a^k - d^k)\\
            0 & \sum_{k=0}^\infty\frac{1}{k!} d^k
        \end{pmatrix}\\
        &= \begin{pmatrix}e^a & b\frac{e^a - e^d}{a - d}\\ 0 & e^d\end{pmatrix}\\
    \end{align*}
    On the other hand, in the case $a = d$ we have that 
    \begin{align*}
        \begin{pmatrix} a & b\\ 0 & a \end{pmatrix}^2
        &= \begin{pmatrix} a & b\\ 0 & a \end{pmatrix}
        \cdot \begin{pmatrix} a & b\\ 0 & a \end{pmatrix}
        = \begin{pmatrix} a^2 & 2ab\\ 0 & a^2 \end{pmatrix}
    \end{align*}
    And that 
    \begin{align*}
        \begin{pmatrix} a & b\\ 0 & a \end{pmatrix}^3
        = \begin{pmatrix} a^2 & 2ab\\ 0 & a^2 \end{pmatrix}
        \cdot \begin{pmatrix} a & b\\ 0 & a \end{pmatrix}
        = \begin{pmatrix} a^3 & 3a^2b\\ 0 & a^3\end{pmatrix}
    \end{align*}
    So we can write that
    \begin{align*}
        \begin{pmatrix} a & b\\ 0 & a \end{pmatrix}^k
        &= \begin{pmatrix} a^k & ka^{k-1}b\\ 0 & a^k\end{pmatrix}
    \end{align*}
    Then $\exp\begin{pmatrix} a & b\\ 0 & a\end{pmatrix}$ in this case gives us
    \begin{align*}
        \exp\begin{pmatrix} a & b\\ 0 & a\end{pmatrix}
        &= \sum_{k=0}^\infty\frac{1}{k!}
        \begin{pmatrix} a^k & ka^{k-1}b\\ 0 & a^k\end{pmatrix}\\
        &= \begin{pmatrix}
            \sum_{k=0}^\infty\frac{1}{k!} a^k &
            b\sum_{k=0}^\infty\frac{ka^{k-1}}{k!}\\
            0 & \sum_{k=0}^\infty\frac{1}{k!} a^k
        \end{pmatrix}\\
        &= \begin{pmatrix}
            \sum_{k=0}^\infty\frac{1}{k!} a^k &
            b\sum_{k=0}^\infty\frac{a^{k-1}}{(k - 1)!}\\
            0 & \sum_{k=0}^\infty\frac{1}{k!} a^k
        \end{pmatrix}\\
        &= \begin{pmatrix}e^a & be^a\\ 0 & e^a\end{pmatrix}
    \end{align*}
    Which also matches with the equation we determined before assuming that
    \begin{align*}
        \lim_{a\to d}\frac{e^a - e^d}{a - d} = e^a
    \end{align*}
    Therefore for any $a,b,d \in \C$ we have that
    \begin{align*}
        \exp\begin{pmatrix} a & b\\ 0 & d\end{pmatrix}
        &= \begin{pmatrix}e^a & b\frac{e^a - e^d}{a - d}\\ 0 & e^d\end{pmatrix}
    \end{align*}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 2.6.6}}
    From Gallier we know that if a matrix $X$ has null trace then $X$ has the
    form
    \begin{align*}
        X = \begin{pmatrix}a & b \\ c & -a\end{pmatrix}
    \end{align*}
    And we saw that these matrices satisfy that
    \begin{align*}
        X^2 = (a^2 + bc)I = -\det(X)I
    \end{align*}
    We saw that if $a^2 + bc = -\det(X) = 0$ then
    \begin{align*}
        e^X = I + X
    \end{align*}
    which matches with the equation
    \begin{align*}
        e^X = \cos(\sqrt{\det{x}})I + \frac{\sin(\sqrt{\det{X}})}{\sqrt{\det{X}}}X
    \end{align*}
    Where we used that $\cos(0) = 1$ and $\lim_{\theta \to 0} \sin\theta/\theta = 1$.\\
    Also, we know that if $a^2 + bc = -\det(X) < 0$ then
    \begin{align*}
        e^X = \cos(\sqrt{\det{X}})I + \frac{\sin(\sqrt{\det{X}})}{\sqrt{\det{X}}}X
    \end{align*}
    which is the equation we have.\\
    Finally, if $a^2 + bc = -\det(X) > 0$ i.e. $\sqrt{\det(X)}$ is complex by
    Gallier we know that
    \begin{align*}
        e^X &= \cosh(\sqrt{\det{x}})I + \frac{\sinh(\sqrt{\det{X}})}{\sqrt{\det{X}}}X\\
        &= \cos(\sqrt{\det{x}})I + \frac{i\sin(\sqrt{\det{X}})}{\sqrt{\det{X}}}X
    \end{align*}
    So for any value of $\det(X)$ the exponential of $X$ is given by the
    equation mentioned above.\\
    Let
    \begin{align*}
        X_1 = \begin{pmatrix}0 & -a\\ a & 0\end{pmatrix}
    \end{align*}
    Then given that $\det(X) = a^2$ we have that
    \begin{align*}
    e^{X_1} &= \cos a\begin{pmatrix}1 & 0\\ 0 & 1\end{pmatrix}
    + \frac{\sin a}{a}\begin{pmatrix}0 & -a\\ a & 0\end{pmatrix}\\
    &= \begin{pmatrix}\cos a & 0\\ 0 & \cos a\end{pmatrix}
    + \begin{pmatrix}0 & -\sin a\\ \sin a & 0\end{pmatrix}\\
    &= \begin{pmatrix}\cos a & -\sin a\\ \sin a & \cos a\end{pmatrix}
\end{align*}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 2.6.7}}
    Let 
    \begin{align*}
        X = \begin{pmatrix} 4 & 3\\ -1 & 2 \end{pmatrix}
    \end{align*}
    Then we can write $X$ as $X = A + B$ where
    \begin{align*}
        A = \begin{pmatrix}
            3 & 0\\ 0 & 3
        \end{pmatrix}
        \qquad
        B = \begin{pmatrix}
            1 & 3\\ -1 & -1
        \end{pmatrix}
    \end{align*}
    We see that they commute cause $A$ can be written as $A = 3I_2$ and $I_2$
    commutes with every matrix then we can compute $e^X$ as follows
    \begin{align*}
        e^X = e^{A + B} = e^{A}e^{B}
    \end{align*}
    First, we need to compute $e^A$
    \begin{align*}
        e^A = \sum_{k \geq 0} \frac{(3I_2)^k}{k!}
        = I_2 \sum_{k \geq 0} \frac{3^k}{k!}
        = e^3 I_2
    \end{align*}
    Now, we compute $e^B$ by using what we have from problem 2.6.6 since $B$
    has $tr(B) = 0$ and $\det(B) = 2$ we have that
    \begin{align*}
        e^B &= \cos(\sqrt{\det{B}})I_2 + \frac{\sin(\sqrt{\det{B}})}{\sqrt{\det{B}}}B\\
        &= \cos(\sqrt{2})I_2 + \frac{\sin(\sqrt{2})}{\sqrt{2}}B
    \end{align*}
    Finally we have that
    \begin{align*}
        e^X &= e^Ae^B\\
        &= (e^3I_2)(\cos(\sqrt{2})I_2 + \frac{\sin(\sqrt{2})}{\sqrt{2}}B)\\
        &= e^3\cos(\sqrt{2})I_2 + e^3\frac{\sin(\sqrt{2})}{\sqrt{2}}B\\
        &= \begin{pmatrix}
            e^3(\cos(\sqrt{2}) + \frac{\sin(\sqrt{2})}{\sqrt{2}})
            & 3e^3\frac{\sin(\sqrt{2})}{\sqrt{2}}\\
            -e^3\frac{\sin(\sqrt{2})}{\sqrt{2}}
            & e^3(\cos(\sqrt{2}) - \frac{\sin(\sqrt{2})}{\sqrt{2}})
        \end{pmatrix}
    \end{align*}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 2.6.9}}
\begin{itemize}
\item [(a)] Let $A$ be unipotent then $A - I$ is nilpotent hence there is
$k \in \N$ such that $(A - I)^k = 0$.
Also, by definition we know that
\begin{align*}
    \log A =\sum_{m=1}^\infty (-1)^{m+1}\frac{(A - I)^m}{m}
\end{align*}
Then we can write that
\begin{align*}
    \log A = (A - I) - \frac{(A - I)^2}{2} + \frac{(A - I)^3}{3}
    - ... + (-1)^{k+1}\frac{(A - I)^k}{k}
\end{align*}
If we compute $(\log A)^2$ the smallest power of $A -I$ is $(A - I)^2$ but also
we get $A-I$ powered to $k +1$, $k+2$, ..., $2k$ which are all zero cause $A-I$
is nilpotent. Then if we compute $(\log A)^k$ the smallest power of $A -I$ is
going to be $(A - I)^k$ which is zero. Therefore $(\log A)^k = 0$ and hence
$\log A$ is nilpotent.
\item [(b)] Let $X$ be nilpotent then there is $k \in \N$ such that $X^k = 0$.
Also, by definition we know that
\begin{align*}
    e^X = \sum_{m=0}^\infty \frac{X^m}{m!}
\end{align*}
But since $X$ is nilpotent we can write that
\begin{align*}
    e^X &= \sum_{m=0}^k \frac{X^m}{m!}\\
    &= I + X + \frac{X^2}{2} + ... + \frac{X^k}{k!}
\end{align*}
Then $e^X - I$ is
\begin{align*}
    e^X - I &= X + \frac{X^2}{2} + ... + \frac{X^k}{k!}
\end{align*}
If we compute $(e^X - I)^2$ the smallest power of $X$ is $X^2$ but also
we get $X$ powered to $k +1$, $k+2$, ..., $2k$ which are all zero cause $X$
is nilpotent. Then if we compute $(e^X - I)^k$ the smallest power of $X$ is
going to be $X^k$ which is zero. Therefore $(e^X - I)^k = 0$ and hence
$e^X - I$ is nilpotent which implies that $e^X$ is unipotent.
\cleardoublepage
\item [(c)] Let $A(t) = I + t(A - I)$ be unipotent then $A(t) - I = t(A - I)$
is nilpotent hence there is some $k \in \N$ for which $t^k(A - I)^k = 0$
then
\begin{align*}
    \log(A(t)) = \sum_{m=1}^k (-1)^{m+1}\frac{t^m(A - I)^m}{m}
\end{align*}
From part (a) we know that $\log(A(t))$ is nilpotent so there is some $j \in \N$
such that $(\log(A(t)))^j = 0$. Hence we can write that
\begin{align*}
    \exp(\log(A(t)))
    &= I + \sum_{n=1}^j\frac{1}{n!}
    \bigg(\sum_{m=1}^k (-1)^{m+1}\frac{t^m(A - I)^m}{m}\bigg)^n
\end{align*}
Then $\exp(\log(A(t)))$ depends polynomially on $t$ and for a sufficiently
small $t$ we can get that $\|A(t) - I\| < 1$ then we can apply Theorem 2.8
and therefore
\begin{align*}
    \exp(\log(A(t))) &= A(t)
\end{align*}
So we have proven that the polynomial $\exp(\log(A(t)))$ is equal to the
polynomial $A(t)$ on an interval close to 0 then they must be equal. This
implies that the result we got must be true for $t=1$ hence
\begin{align*}
    \exp(\log(A)) = \exp(\log(A(1))) &= I + 1(A -I) = A
\end{align*}


In the same way, let now $X(t) = tX$ be nilpotent then there is some
$k \in \N$ such that $X(t)^k = (tX)^k = 0$ then 
\begin{align*}
    \exp(X(t)) = \sum_{m=0}^k \frac{t^mX^m}{m!}
\end{align*}
From part (b) we know that $\exp(X(t))$ is unipotent so there is some $j \in \N$
such that $(\exp(X(t)) - I)^j = 0$. Hence we can write that
\begin{align*}
    \log(\exp(X(t)))
    &= \sum_{n=1}^j\frac{(-1)^{n+1}}{n}
    \bigg(\sum_{m=0}^k \frac{t^mX^m}{m!} -I\bigg)^n
\end{align*}
Again $\log(\exp(X(t)))$ depends polynomially on $t$ and for a sufficiently
small $t$ we can get that $\|X(t)\| < \log 2$ and hence $\|e^X - I\| < 1$ then
we can apply Theorem 2.8 and therefore
\begin{align*}
    \log(\exp(X(t))) &= X(t)
\end{align*}
Finally, again because the two polynomials are equal on an interval close to
0 then must be that
\begin{align*}
    \log(\exp(X)) = \log(\exp(X(1))) &= 1X = X
\end{align*}

\end{itemize}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 2.6.10}}
    Let $A$ be an invertible $n \times n$ matrix then $A$ is similar to a
    block-diagonal matrix $D$ where each block is of the form
    $\lambda I + N_\lambda$ with $N_\lambda$ nilpotent. Then
    \begin{align*}
        A = PDP^{-1}
    \end{align*}
    for some invertible matrix $P$.
    Let $D'$ be a matrix defined as
    \begin{align*}
        D' &= \begin{pmatrix}
            \log(\lambda_1I(I + N_{\lambda_1}/\lambda_1)) & 0 & ... & 0 \\
            0 & \log(\lambda_2I (I + N_{\lambda_2}/\lambda_2)) & ... & 0 \\
            \vdots & \vdots & \vdots & \vdots\\
            0 & 0 & ... & \log(\lambda_nI(I + N_{\lambda_n}/\lambda_n))
        \end{pmatrix}\\
        &= \begin{pmatrix}
            \log(\lambda_1I) + \log(I + N_{\lambda_1}/\lambda_1) & 0 & ... & 0 \\
            0 & \log(\lambda_2I) + \log(I + N_{\lambda_2}/\lambda_2) & ... & 0 \\
            \vdots & \vdots & \vdots & \vdots\\
            0 & 0 & ... & \log(\lambda_nI)+ \log(I + N_{\lambda_n}/\lambda_n)
        \end{pmatrix}
    \end{align*}
    Where we used that $\lambda_1I$ and $I + N_{\lambda_1}/\lambda_1$ commutes.
    So
    \begin{align*}
        e^{D'} &= \begin{pmatrix}
            \exp(\log(\lambda_1I) + \log(I + N_{\lambda_1}/\lambda_1)) & ... & 0 \\
            0  & ... & 0 \\
            \vdots & \vdots & \vdots\\
            0 & ... & \exp(\log(\lambda_nI)+ \log(I + N_{\lambda_n}/\lambda_n))
        \end{pmatrix}\\
        &= \begin{pmatrix}
            \exp(\log(\lambda_1I))\exp(\log(I + N_{\lambda_1}/\lambda_1)) & ... & 0 \\
            0  & ... & 0 \\
            \vdots & \vdots & \vdots\\
            0 & ... & \exp(\log(\lambda_nI)) \exp(\log(I + N_{\lambda_n}/\lambda_n))
        \end{pmatrix}
    \end{align*}
    Since $I + N_{\lambda_i}/\lambda_i$ is unipotent we know that
    $\exp(\log(I + N_{\lambda_i}/\lambda_i)) = I + N_{\lambda_i}/\lambda_i$.
    And for $\exp(\log(\lambda_iI))$ we see that 
    \begin{align*}
        \exp(\log(\lambda_iI))
        &= \exp\bigg(\sum_{m = 1}^\infty (-1)^{m+1} \frac{(\lambda_i I - I)^m}{m}\bigg)\\
        &= \exp\bigg(\sum_{m = 1}^\infty (-1)^{m+1} \frac{(\lambda_i-1)^m}{m}~I\bigg)\\
        &= \exp(\log(\lambda_i)I)\\
        &= \sum_{m=0}^\infty \frac{(\log(\lambda_i)I)^m}{m!}\\
        &= \sum_{m=0}^\infty \frac{\log(\lambda_i)^m}{m!}~I\\
        &= \exp(\log(\lambda_i))I\\
        &= \lambda_iI
    \end{align*}
    Then
    \begin{align*}
        e^{D'} &= \begin{pmatrix}
            \lambda_1I(I + N_{\lambda_1}/\lambda_1) & 0 & ... & 0 \\
            0 & \lambda_2I(I + N_{\lambda_2}/\lambda_2) & ... & 0 \\
            \vdots & \vdots & \vdots & \vdots\\
            0 & 0 & ... & \lambda_n I(I + N_{\lambda_n}/\lambda_n)
        \end{pmatrix}\\
        &= \begin{pmatrix}
            \lambda_1I + N_{\lambda_1} & 0 & ... & 0 \\
            0 & \lambda_2I + N_{\lambda_2} & ... & 0 \\
            \vdots & \vdots & \vdots & \vdots\\
            0 & 0 & ... & \lambda_n I + N_{\lambda_n}
        \end{pmatrix}
    \end{align*}
    Hence $D = e^{D'}$ and therefore
    \begin{align*}
        A = PDP^{-1} = Pe^{D'}P^{-1} = e^{PD'P^{-1}}
    \end{align*}
    Which implies that $A$ can be written as $A = e^{X}$ where $X = PD'P^{-1}$.

\end{proof}
\cleardoublepage
\subsection*{2.2 The Lie Groups $\bm{GL}(n, \R)$, $\bm{SL}(n, \R)$, $\bm{O}(n)$,
$\bm{SO}(n)$, the Lie Algebras $\mathfrak{gl}(n, \R)$, $\mathfrak{sl}(n, \R)$,
$\mathfrak{o}(n)$, $\mathfrak{so}(n)$, and the Exponential Map}
\begin{proof}{\textbf{Problem 2.2.}}
    Let
    \begin{align*}
        B = \begin{pmatrix}
            a & b\\ c &-a
        \end{pmatrix} \in \mathfrak{sl}(2, \C)
    \end{align*}
    also, let $\omega^2 = a^2 + bc$ such that $\omega \neq 0$.
    We see that $B^2$ is
    \begin{align*}
        B^2 = \begin{pmatrix}a & b\\ c &-a \end{pmatrix}
        \cdot \begin{pmatrix} a & b\\ c &-a \end{pmatrix}
        = \begin{pmatrix} a^2 + bc & 0\\ 0 & a^2 +bc \end{pmatrix}
        = (a^2 + bc)I_2
    \end{align*}
    Then
    \begin{align*}
        e^B &= I_2 + \frac{B}{1!} + \frac{\omega^2}{2!}I_2
        + \frac{\omega^2}{3!}B + \frac{\omega^4}{4!}I_2 + \frac{\omega^4}{5!}B
        + ...\\
        &= \bigg(1 + \frac{\omega^2}{2!} + \frac{\omega^4}{4!} + ...\bigg)I_2
        +  \bigg(1 + \frac{\omega^2}{3!} + \frac{\omega^4}{5!} + ...\bigg)B\\
        &= \bigg(1 + \frac{\omega^2}{2!} + \frac{\omega^4}{4!} + ...\bigg)I_2
        + \bigg(\omega + \frac{\omega^3}{3!} + \frac{\omega^5}{5!} + ...\bigg)
        \frac{B}{\omega}\\
        &= \cosh\omega I_2 + \frac{\sinh\omega}{\omega}B
    \end{align*}
    Suppose now that $\omega^2 = 0 = a^2 + bc$ then $B^2 = 0$ and hence
    \begin{align*}
        e^B &= I_2 + \frac{B}{1!} = I_2 + B
    \end{align*}
    Finally, suppose the matrix
    \begin{align*}
        \begin{pmatrix}-1 & 1\\ 0 & -1 \end{pmatrix}
    \end{align*}
    is the exponential of a matrix in $\mathfrak{sl}(2,\C)$, we want to arrive
    to a contradiction. Then must be that
    \begin{align*}
        \exp\begin{pmatrix} a & b\\ c &-a \end{pmatrix}
        = \begin{pmatrix}-1 & 1\\ 0 & -1 \end{pmatrix}
    \end{align*}
    If $\omega \neq 0$ where $\omega^2 = a^2 + bc$ then must be that
    \begin{align*}
        \begin{pmatrix}
            \cosh\omega + \frac{\sinh\omega}{\omega}a & \frac{\sinh\omega}{\omega}b\\
            \frac{\sinh\omega}{\omega}c & \cosh\omega - \frac{\sinh\omega}{\omega}a
        \end{pmatrix}
        = \begin{pmatrix}-1 & 1\\ 0 & -1 \end{pmatrix}
    \end{align*}
    Then $c = 0$, $b = \frac{\omega}{\sin\omega}$ and
    \begin{align*}
        \cosh\omega + \frac{a}{b} = -1\\
        \cosh\omega - \frac{a}{b} = -1
    \end{align*}
    But there is no solution to these equations.
    So there must be that $\omega = 0$ and hence
    \begin{align*}
        \begin{pmatrix} 1 + a & b\\ c & 1-a \end{pmatrix}
        = \begin{pmatrix}-1 & 1\\ 0 & -1 \end{pmatrix}
    \end{align*}
    Then $c = 0$, $b = 1$ and
    \begin{align*}
        1 + a = -1\\
        1 - a = -1
    \end{align*}
    But again there is no solution to these equations.\\
    Therefore we arrived at a contradiction and must be that there is no
    exponential matrix $A$ in $\mathfrak{sl}(2,\C)$ such that
    \begin{align*}
        e^A = \begin{pmatrix}-1 & 1\\ 0 & -1 \end{pmatrix}
    \end{align*}
    Which implies that the map $\exp: \mathfrak{sl}(2,\C) \to \bm{SL}(2,\C)$ is
    not surjective.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Problem 2.3.}}
\begin{itemize}
    \item [(a)] Let $B = \log(I - N)$. We see that $(I - N) - I = -N$ is
    nilpotent, this implies that $I -N$ is unipotent then by the problem
    "Hall: 2.6.9(c)" we know that $\exp(\log(I - N)) = I -N$ therefore
    \begin{align*}
        \exp(B) = \exp(\log(I - N)) = I - N = A
    \end{align*}
    \item [(b)] Let $A \in \bm{GL}(n, \C)$ then $A$ is a complex invertible
    $n \times n$ matrix and by "Hall: 2.6.10" we know there is
    $B \in \mathfrak{gl}(n, \C)$ such that $e^B = A$. Thus,
    the exponential map $\exp :\mathfrak{gl}(n,\C)\to \bm{GL}(n,\C)$
    is surjective.
\end{itemize}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Problem 2.4.}}
\begin{itemize}
    \item [(a)] Let 
    \begin{align*}
        A = \begin{pmatrix}
            0 & -c & b\\
            c & 0 & -a\\
            -b & a & 0
        \end{pmatrix} \in \mathfrak{so}(3)
    \end{align*}
    Then
    \begin{align*}
        A^2 &= \begin{pmatrix}
            0 & -c & b\\
            c & 0 & -a\\
            -b & a & 0
        \end{pmatrix}
        \cdot \begin{pmatrix}
            0 & -c & b\\
            c & 0 & -a\\
            -b & a & 0
        \end{pmatrix}\\
        &= \begin{pmatrix}
            -c^2 -b^2 & ba & ac\\
            ba & -c^2-a^2 & bc\\
            ac & bc & -b^2-a^2
        \end{pmatrix}\\
        &= \begin{pmatrix}
            a^2-a^2-b^2-c^2 & ba & ac\\
            ba & b^2-a^2-b^2-c^2 & bc\\
            ac & bc & c^2-a^2-b^2-c^2
        \end{pmatrix}\\
        &= \begin{pmatrix}
            -a^2-b^2-c^2 & 0 & 0\\
            0 & -a^2-b^2-c^2 & 0\\
            0 & 0 & -a^2-b^2-c^2
        \end{pmatrix} + \begin{pmatrix}
            a^2 & ba & ac\\
            ba & b^2 & bc\\
            ac & bc & c^2
        \end{pmatrix}\\
        &= (-a^2-b^2-c^2)I + B\\
        &=-\theta^2I + B
    \end{align*}
    Where $\theta = \sqrt{a^2 + b^2 + c^2}$ and
    \begin{align*}
        B = \begin{pmatrix}
            a^2 & ba & ac\\
            ba & b^2 & bc\\
            ac & bc & c^2
        \end{pmatrix}
    \end{align*}
    On the other hand, we see that
    \begin{align*}
        AB &= \begin{pmatrix}
            0 & -c & b\\
            c & 0 & -a\\
            -b & a & 0
        \end{pmatrix}
        \cdot \begin{pmatrix}
            a^2 & ba & ac\\
            ba & b^2 & bc\\
            ac & bc & c^2
        \end{pmatrix}\\
        &= \begin{pmatrix}
            -abc + abc & -b^2c + b^2c & -bc^2 + bc^2\\
            a^2c - a^2c & abc - abc & ac^2 - ac^2\\
            -a^2b + a^2b & -ab^2 + ab^2 & -abc + abc
        \end{pmatrix}\\
        &= 0
    \end{align*}
    Also
    \begin{align*}
        BA &= \begin{pmatrix}
            a^2 & ba & ac\\
            ba & b^2 & bc\\
            ac & bc & c^2
        \end{pmatrix}
        \cdot \begin{pmatrix}
            0 & -c & b\\
            c & 0 & -a\\
            -b & a & 0
        \end{pmatrix}\\
        &= \begin{pmatrix}
            abc - abc & -a^2c + a^2c & ba^2 - ba^2\\
            b^2c - b^2c & -abc + abc & ab^2 - ab^2\\
            b^2c - b^2c & -ac^2 + ac^2 & abc - abc
        \end{pmatrix}\\
        &= 0
    \end{align*}
    Therefore $AB = BA = 0$. Finally, we compute $A^3$ as follows
    \begin{align*}
        A^3 = A(-\theta^2I + B) = -A\theta^2I + AB = -\theta^2A
    \end{align*}
    \item [(b)] This is proved in Proposition 2.7.
    \item [(c)] Since $\det(e^A) = e^{\text{tr}(A)}$ and we see that
    $\text{tr}(A) = 0$ then
    $$\det(e^A) = e^0 = 1$$
    On the other hand, we know that $(e^A)^T = e^{A^T}$ also, $A^T = -A$ and 
    hence $A$ and $A^T$ commute then
    \begin{align*}
        (e^A)^T e^A = e^{A^T} e^A = e^{A^T + A} = e^{-A + A} = e^0 = I
    \end{align*}
    And 
    \begin{align*}
        e^A(e^A)^T = e^Ae^{A^T} = e^{A + A^T} = e^{A - A} = e^0 = I
    \end{align*}
    Therefore $e^A$ is an orthogonal matrix of determinant +1.
    \item [(d)]
    \begin{itemize}
    \item [(2)] Let $R \in \mathbf{SO}(3)$ as
    \begin{align*}
        R = \begin{pmatrix}
            \cos\phi & -\sin\phi & 0 \\
            \sin\phi & \cos\phi & 0 \\
            0 & 0 & 1 \\
        \end{pmatrix}
    \end{align*}
    And let $B \in \mathfrak{so}(3)$
    such that $B$ is of the form
    \begin{align*}
        B &= \left\{\frac{\theta}{2\sin\theta}(R - R^T)
        ~:~1 + 2 \cos\theta = \tr(R)\right\}\\
        &= \frac{\theta}{2\sin\theta}
         \begin{pmatrix}
            \cos\phi & -\sin\phi & 0 \\
            \sin\phi & \cos\phi & 0 \\
            0 & 0 & 1 \\
        \end{pmatrix} - \begin{pmatrix}
            \cos\phi & \sin\phi & 0 \\
            -\sin\phi & \cos\phi & 0 \\
            0 & 0 & 1 \\
        \end{pmatrix}\\
        &= \frac{\theta}{2\sin\theta}
         \begin{pmatrix}
            0 & -2\sin\phi & 0 \\
            2\sin\phi & 0 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}
    \end{align*}
    Given that $1 + 2\cos\theta = \tr(R)$ we get that
    $1 + 2\cos\theta = 2\cos\phi + 1$ hence $\theta = \phi$ then we write
    $B$ as
    \begin{align*}
        B &= \begin{pmatrix}
            0 & -\theta & 0 \\
            \theta & 0 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}
    \end{align*}
    Then $B^2$ is
    \begin{align*}
        B^2 &= \begin{pmatrix}
            0 & -\theta & 0 \\
            \theta & 0 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}\cdot \begin{pmatrix}
            0 & -\theta & 0 \\
            \theta & 0 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix} = \begin{pmatrix}
            -\theta^2 & 0 & 0 \\
            0 & -\theta^2 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}
    \end{align*}
    So we compute $e^B$ as follows
    \begin{align*}
        e^B &= I_3 + \frac{\sin\theta}{\theta} B + \frac{(1 - \cos\theta)}{\theta^2}B^2\\
        &= I_3 + \frac{\sin\theta}{\theta}\begin{pmatrix}
            0 & -\theta & 0 \\
            \theta & 0 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}
        + \frac{(1 - \cos\theta)}{\theta^2}\begin{pmatrix}
            -\theta^2 & 0 & 0 \\
            0 & -\theta^2 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}\\
        &= I_3 + \begin{pmatrix}
            0 & -\sin\theta & 0 \\
            \sin\theta & 0 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}
        + \begin{pmatrix}
            \cos\theta - 1 & 0 & 0 \\
            0 & \cos\theta - 1 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}\\
        &=  \begin{pmatrix}
            \cos\theta & -\sin\theta & 0 \\
            \sin\theta & \cos\theta & 0 \\
            0 & 0 & 1 \\
        \end{pmatrix}
    \end{align*}
    Therefore we have proven that $e^B = R$.
    \item [(3)] Let now $R \neq I$ and $\tr(R) = -1$ then from the trace
    equation we see that 
    \begin{align*}
        2\cos\phi + 1 &= -1\\
        \cos\phi + 1 &= 0\\
        \phi &= \arccos(-1) = \pi
    \end{align*}
    We can compute the eigenvalues of $R$ from the characteristic polynomial
    as follows
    \begin{align*}
        \det(R - \lambda I) = 0\\
        \begin{vmatrix}
            \cos\phi - \lambda & -\sin\phi & 0\\
            \sin\phi & \cos\phi - \lambda & 0\\
            0 & 0 & 1 - \lambda
        \end{vmatrix} = 0\\
        (1 - \lambda)(\cos\phi - \lambda)^2 + \sin^2\phi(1- \lambda) = 0\\
        (1 - \lambda)(\cos^2\phi - 2\lambda\cos\phi + \lambda^2 + \sin^2\phi) = 0\\
        (1 - \lambda)(\lambda^2 - 2\lambda\cos\phi + 1) = 0\\
        (1 - \lambda)(\lambda^2 + 2\lambda + 1) = 0
    \end{align*}
    From the first factor we see that one eigenvalue of $R$ is $1$ and from the
    second factor we have a double eigenvalue at $-1$.

    Given that $\phi = \pi$ then $R$ becomes
    \begin{align*}
        R = \begin{pmatrix}
            -1 & 0 & 0\\
            0 & -1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
    \end{align*}
    Then we see that $R = R^T$ and also $R^2$ is
    \begin{align*}
        R^2 = \begin{pmatrix}
            -1 & 0 & 0\\
            0 & -1 & 0\\
            0 & 0 & 1
        \end{pmatrix} \cdot \begin{pmatrix}
            -1 & 0 & 0\\
            0 & -1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
         = \begin{pmatrix}
            1 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & 1
        \end{pmatrix} = I_3
    \end{align*}
\cleardoublepage
    Let $S = \frac{1}{2}(R - I)$ then replacing $R$ and solving we get that
    \begin{align*}
        S = \frac{1}{2} \begin{pmatrix}
            -2 & 0 & 0 \\
            0 & -2 & 0\\
            0 & 0 & 0
        \end{pmatrix}
        = \begin{pmatrix}
            -1 & 0 & 0 \\
            0 & -1 & 0\\
            0 & 0 & 0
        \end{pmatrix}
    \end{align*}
    So $S$ is symmetric and since it's a diagonal matrix the eigenvalues of $S$
    are $-1, -1, 0$.

    Suppose there is a skew symmetric matrix $U$ defined as follows
    \begin{align*}
        U = \begin{pmatrix}
            0 & -d & c\\
            d & 0 & -b\\
            -c & b & 0
        \end{pmatrix}
    \end{align*}
    Then $U^2$ is
    \begin{align*}
        U^2 &= \begin{pmatrix}
            0 & -d & c\\
            d & 0 & -b\\
            -c & b & 0
        \end{pmatrix}\cdot
        \begin{pmatrix}
            0 & -d & c\\
            d & 0 & -b\\
            -c & b & 0
        \end{pmatrix}\\
        &= \begin{pmatrix}
            -c^2-d^2 & bc & bd\\
            bc & -b^2-d^2 & cd\\
            bd & cd & -b^2 -c^2
        \end{pmatrix}
    \end{align*}
    So if $b = c = 0$ and $d = -1$ we get that
    \begin{align*}
        U^2 &= \begin{pmatrix}
            -1 & 0 & 0\\
            0 & -1 & 0\\
            0 & 0 & 0
        \end{pmatrix} = S
    \end{align*}
    If $U^2 = S$ then we see that $\tr(S) = -2$ then must be that
    \begin{align*}
        -c^2 - d^2 -b^2 - d^2 -b^2 - c^2 &= -2\\
        -2b^2 -2c^2 -2d^2  &= -2\\
        b^2 + c^2 + d^2  &= 1
    \end{align*}
\cleardoublepage
    Finally, let
    \begin{align*}
        B = \left\{(2k + 1)\pi\begin{pmatrix}
            0 & 1 & 0\\
            -1 & 0 & 0\\
            0 & 0 & 0
        \end{pmatrix} : k\in \Z\right\}
    \end{align*}
    We want to prove that $e^B = R$ then
    \begin{align*}
        e^B &= I_3 + \frac{\sin\theta}{\theta} B + \frac{(1 - \cos\theta)}{\theta^2}B^2\\
        &= I_3 + \frac{2}{\pi^2}B^2\\
        &= I_3 + \frac{2}{\pi^2}(2k + 1)^2\pi^2\begin{pmatrix}
            -1 & 0 & 0\\
            0 & -1 & 0\\
            0 & 0 & 0
        \end{pmatrix}\\
        &= (2k + 1)^2\begin{pmatrix}
            1-2 & 0 & 0\\
            0 & 1-2 & 0\\
            0 & 0 & 1
        \end{pmatrix}\\
        &= \begin{pmatrix}
            -1 & 0 & 0\\
            0 & -1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
    \end{align*}
    Where we took $k = 0$ in the last step.
    \end{itemize}
\end{itemize}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Duistermaat: 2.4}}
    Let $A \in \End(\R^n)$ be an orthogonal transformation i.e.
    $\|Ax\| = \|x\|$ for all $x\in \R^n$
    \begin{itemize}
        \item [(i)] Let $Ax = 0$ then $\|Ax\| = 0$ but this implies that
        $\|x\| = 0$ hence must be that $x = 0$. Then $A^{-1}$ exists.
        
        Now, let $x_1, x_2 \in \R^n$ and suppose $Ax_1 = Ax_2$ then
        $A(x_1 - x_2) = 0$ since $A \in \End(\R^n)$ but this implies that
        $x_1 - x_2 = 0$ by what we proved before then $x_1 = x_2$
        and therefore $A$ is injective.

        Given that $A:\R^n \to \R^n$ then $\dim(\dom(A)) = \dim(\range(A))= n$
        hence $\dim(\nullsp(A)) = 0$ because of the rank-nullity theorem.
        Also, given that $A^{-1}$ exists this implies that $\range(A) = \R^n$
        i.e. $A$ is surjective.
        
        Therefore $A$ is bijective and hence $A \in \Aut(\R^n)$.

        Finally, let us take $x = A^{-1}y$ then
        $$\|Ax\| = \|AA^{-1}y\| = \|y\| = \|A^{-1}y\| = \|x\|$$
        Therefore $A^{-1}$ is an orthogonal transformation.

        \item [(ii)] Let us note first that
        $\|Ax + Ay\| = \|A(x + y)\| = \|x + y\|$
        then by the Polarization identity we have that
        \begin{align*}
            \langle Ax, Ay\rangle
            &= \frac{1}{4}(\|Ax + Ay\|^2 - \|Ax - Ay\|^2)\\
            &= \frac{1}{4}(\|x + y\|^2 - \|x - y\|^2)\\
            &= \langle x, y\rangle
        \end{align*}
        On the other hand, let us name $Ax = z$ then by the adjoint linear
        operator characteristic property we have that
        \begin{align*}
            \langle A^tz, y\rangle = \langle z, (A^t)^ty\rangle
            = \langle z, Ay\rangle
        \end{align*}
        so replacing $z$ again we have that
        \begin{align*}
            \langle A^tAx, y\rangle = \langle Ax, Ay\rangle
        \end{align*}
        Therefore joining both results we see that
        \begin{align*}
            \langle A^tAx, y\rangle = \langle Ax, Ay\rangle = \langle x, y\rangle
        \end{align*}
\cleardoublepage
        \item [(iii)] From the previous result we see that
        \begin{align*}
            \langle A^tAx - x, y\rangle = \langle A^tAx, y\rangle - \langle x, y\rangle = 0
        \end{align*}
        for all $x$ and $y$ in $\R^n$. So taking $y = A^tAx-x$ we see by the
        Polarization identity that
        \begin{align*}
        \langle A^tAx - x, A^tAx - x\rangle = \|A^tA(x - x)\|^2 = 0
        \end{align*}
        Hence by the properties of the norm must be that 
        \begin{align*}
            A^tAx - x &= 0\\
            A^tAx = x
        \end{align*}
        Therefore $A^tA = I$.
        
        By the properties of the determinant we see that
        \begin{align*}
            \det(A^tA) = \det(A^t)\det(A) = \det(A)\det(A) = \det(A)^2
        \end{align*}
        But also we know that $\det(I) = 1$ hence $\det(A)^2 = 1$
        so must be that $\det(A) = \pm 1$.

        From (i) we know that $A^{-1}$ exists so we have that $A^{-1}A = I$
        but then $A^{-1}A = A^tA$ so $A^{-1} = A^t$ and we saw in (i) that
        $A^{-1}$ is orthogonal hence $A^t$ is orthogonal.
        In the same way, as above we see that
        $\langle (A^t)^tA^tx - x, y\rangle = \langle AA^tx - x, y\rangle = 0$
        for all $x,y\in \R^n$ therefore this implies that $AA^t = I$.

        Let $(e_1, ..., e_n)$ be the standard basis for $\R^n$ then by
        what we proved in (ii) we have that
        $\langle Ae_i, Ae_j\rangle = \langle e_i, e_j\rangle = \delta_{ij}$
        but since $A^t$ is orthogonal then it has the same property i.e.
        $\langle A^te_i, A^te_j\rangle = \langle e_i, e_j\rangle = \delta_{ij}$
        So joining results we have that
        \begin{align*}
            \langle Ae_i, Ae_j\rangle = \langle A^te_i, A^te_j\rangle
            = \langle e_i, e_j\rangle = \delta_{ij}
        \end{align*}
        Therefore since $\|Ae_i\| = \|A^te_i\| = \|e_i\| = 1$ and
        \begin{align*}
            \langle Ae_i, Ae_j\rangle = \langle A^te_i, A^te_j\rangle = 0
        \end{align*}
        if $i \neq j$ then both column and row vectors of $A$ form an
        orthonormal basis for $\R^n$.
\cleardoublepage
        \item [(iv)] If $A \in \bf{GL}(n, \R)$ and $A$ is orthogonal from (ii)
        and (iii) we get that $A^tA = I$ i.e. $A \in \bf{O}(n, \R)$.

        We see that the $ij$ element of $AA^t = I$ is given by
        \begin{align*}
           a_{i1}a^t_{1j} + a_{i2}a^t_{2j} + ... + a_{in}a^t_{nj}
           = \sum_{1\leq k\leq n} a_{ik}a^t_{kj} = \delta_{ij}
        \end{align*}
        Where $a^t_{kj}$ are the elements of $A^t$.
        Also, we see that $a^t_{kj} = a_{jk}$ then
        \begin{align*}
            a_{i1}a_{j1} + a_{i2}a_{j2} + ... + a_{in}a_{jn}
            = \sum_{1\leq k\leq n} a_{ik}a_{jk} = \delta_{ij}
        \end{align*}
        On the other hand, if we compute $A^tA = I$ the $ij$ element is given by
        \begin{align*}
            a^t_{i1}a_{1j} + a^t_{i2}a_{2j} + ... + a^t_{in}a_{nj}
            = \sum_{1\leq k\leq n} a^t_{ik}a_{kj} = \delta_{ij}
        \end{align*}
        But again since $a^t_{ik} = a_{ki}$ we have that 
        \begin{align*}
            a_{1i}a_{1j} + a_{2i}a_{2j} + ... + a_{ni}a_{nj}
            = \sum_{1\leq k\leq n} a_{ki}a_{kj} = \delta_{ij}
        \end{align*}
        Finally, joining both results we have that
        \begin{align*}
            \sum_{1\leq k\leq n} a_{ki}a_{kj}
            = \sum_{1\leq k\leq n} a_{ik}a_{jk} = \delta_{ij}
        \end{align*} 
    \end{itemize}   
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Duistermaat: 2.5}}
    Let $R \in \bm{SO}(3, \R)$, $\alpha \in \R$ such that
    $0 \leq \alpha \leq \pi$ and $a \in \R^3$ with $\|a\| = 1$.\\
    Following the hint given, let $b \in N_a$ with $\|b\| = 1$ and define
    $c = a \times b$.

    First, we want to check that $Rb \in N_a$ so we want that
    $\langle Rb, a \rangle = 0$. We see that
    \begin{align*}
        \langle Rb, a \rangle = \langle Rb, Ra \rangle = \langle b, a\rangle
    \end{align*}
    Where we used in the first equality that $Ra = a$ and in the second
    equality we used the property determined in the previous problem.
    But $b \in N_a$ hence must be that $\langle b, a \rangle = 0$ therefore 
    $\langle Rb, a \rangle = 0$ and $Rb \in N_a$.

    Let $\|Rb\| = 1$ then since $Rb \in N_a$ we can write that
    $Rb = C b + D c$ for some scalars $C,D$ so we have that
    \begin{align*}
        \|Rb\|^2 &= 1\\
        \langle Rb, Rb\rangle &= 1\\
        \langle Cb + Dc, Cb + Dc\rangle &= 1\\
        C^2\langle b, b\rangle + CD\langle b, c\rangle
        + DC\langle c, b\rangle + D^2\langle c, c\rangle &= 1\\
        C^2 + D^2 &= 1
    \end{align*}
    Where we used that $\langle b, b\rangle = 1$ and $\langle b, c\rangle = 0$.
    Therefore must be that $C$ and $D$ are of the form $C = \cos\alpha$ and
    $D = \sin\alpha$ and hence there exists $0\leq \alpha\leq \pi$ such that
    $Rb = (\cos\alpha) b + (\sin\alpha) c$.

    On the other hand, let $Rc \in N_a$, $\|Rc\| = 1$, $Rc \perp Rb$ and
    $\det R = 1$. Then we can write $Rc = Ab + Bc$ for some scalars $A,B$.
    By the same procedure we showed above we see that $A^2 + B^2 = 1$ but
    since $Rc \perp Rb$ must be that $A = -\sin\alpha$ and $B = \cos\alpha$
    therefore we have that $Rc = -(\sin\alpha) b + (\cos\alpha) c$.

    Finally, we want to show that for any $y \in N_a$ we have that
    $\det(a~y~Ry) > 0$. If we take the basis $(a,b,c)$ for $\R^3$ then we have
    that
    \begin{align*}
        &\det\begin{pmatrix}
            a & y_1 & y_1\\
            0 & y_2 & y_2\cos\alpha - y_3\sin\alpha\\
            0 & y_3 & y_2\sin\alpha + y_3\cos\alpha\\
        \end{pmatrix} = \\
        &\qquad= ay_2(y_2\sin\alpha + y_3\cos\alpha)
        - ay_3(y_2\cos\alpha - y_3\sin\alpha)\\
        &\qquad= a\sin\alpha(y_2^2 + y_3^2)
    \end{align*}
    Then since we can take $a > 0$ and
    $0 \leq \alpha \leq \pi$ we have that $$\det(a~y~Ry) > 0$$
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Duistermaat: 4.22}}
\begin{itemize}
    \item [(i)] Let $x \in \R^3$ and $y$ be component of $x$ perpendicular to 
    $a$. Let us also define $b$ in the direction of $y$ such that $\|b\| = 1$
    then $a \perp b$ and hence we can write that $y = \|y\|b$.

    Then we can write that
    \begin{align*}
        x = Ca + \|y\|b + 0\cdot (a \times b) = Ca + y
    \end{align*}
    So with respect to the basis $(a, b, a\times b) \in \R^3$ by definition we
    have that
    \begin{align*}
        \langle x, a\rangle &= C\cdot 1 + \|y\| \cdot 0 + 0\\
        C &= \langle x, a\rangle
    \end{align*}
    Therefore
    \begin{align*}
        x = \langle x, a\rangle a + y
    \end{align*}
    \item [(ii)] By definition $a \times x$ is a vector such that 
    $\langle a, a \times x \rangle = \det(a~a~x)$
    but then by the properties of determinants since two columns are the same
    this implies that $\langle a, a \times x \rangle = 0$ then
    $a\times x$ is in $N_a$.

    Also, since $a\times x = a \times y$ we have that
    $\langle y, a \times y \rangle = \det(y~a~y) = 0$ which implies that
    $a \times x$ is also perpendicular to $y$.

    Finally, from Section 5.3 we know that
    \begin{align*}
        \langle a, y \rangle^2 + \|a \times y\|^2 = \|a\|^2\|y\|^2
    \end{align*}
    But $\langle a, y \rangle = 0$ since $y$ is perpendicular to $a$ and
    $\|a\| = 1$ therefore
    \begin{align*}
        \|a \times y\| = \|y\|
    \end{align*}
    \item [(iii)] Let us compute $R_{\alpha, a} y$ as follows
    \begin{align*}
        \begin{pmatrix}
            1 & 0 & 0\\
            0 & \cos\alpha & -\sin\alpha\\
            0 & \sin\alpha & \cos\alpha
        \end{pmatrix}
        \cdot \begin{pmatrix}
            0 \\ \|y\| \\ 0
        \end{pmatrix}
        = \begin{pmatrix}
            0 \\ (\cos\alpha) \|y\| \\ (\sin\alpha) \|y\|
        \end{pmatrix}
    \end{align*}
    Hence since $a \times x$ is a vector of length $\|y\|$ perpendicular to $y$
    we can write that
    \begin{align*}
        R_{\alpha,a} y = (\cos\alpha)y + (\sin\alpha)a \times x
    \end{align*}
    Also, since $y =  x - \langle x, a\rangle a$ we have that
    \begin{align*}
        R_{\alpha,a} (x - \langle x, a\rangle a)
        &= (\cos\alpha)x - (\cos\alpha)\langle x, a\rangle a + (\sin\alpha)a \times x\\
        R_{\alpha,a} x &= R_{\alpha,a} \langle x, a\rangle a + 
        (\cos\alpha)x - (\cos\alpha)\langle x, a\rangle a + (\sin\alpha)a \times x\\
        R_{\alpha,a} x &= \langle x, a\rangle a + 
        (\cos\alpha)x - (\cos\alpha)\langle x, a\rangle a + (\sin\alpha)a \times x\\
        R_{\alpha,a} x
        &= (1 - \cos\alpha)\langle x, a\rangle a + (\cos\alpha)x + (\sin\alpha)a \times x
    \end{align*}
    Where we used that $R_{\alpha,a} \langle x, a\rangle a = \langle x, a\rangle a$.

    Let $a_1,a_2,a_3$ be the components of $a$ in the standard basis of
    $\R^3$. We name the basis vectors of the standard basis as $e_1, e_2, e_3$.
    
    Let us compute $R_{\alpha,a} e_1$ as follows
    \begin{align*}
        R_{\alpha,a} e_1 &= (1 - \cos\alpha)\langle e_1, a\rangle a
        + (\cos\alpha)e_1 + (\sin\alpha)a \times e_1\\
        &= (1 - \cos\alpha)a_1 (a_1e_1 + a_2e_2 + a_3e_3) + (\cos\alpha)e_1\\
        &\quad + (\sin\alpha)(a_1e_1 + a_2e_2 + a_3e_3) \times e_1\\
        &= (\cos\alpha + a_1^2(1 - \cos\alpha))e_1 
        + (a_3(\sin\alpha) + a_1a_2(1 - \cos\alpha)) e_2\\
        &\quad + (-a_2(\sin\alpha) + a_1a_3(1 - \cos\alpha))e_3\\
        &= (\cos\alpha + a_1^2c(\alpha))e_1 
        + (a_3(\sin\alpha) + a_1a_2c(\alpha)) e_2\\
        &\quad + (-a_2(\sin\alpha) + a_1a_3c(\alpha))e_3
    \end{align*}
    Where $c(\alpha) = 1 - \cos\alpha$. In the same way, we compute
    $R_{\alpha, a}e_2$ and $R_{\alpha, a}e_3$ as follows
    \begin{align*}
        R_{\alpha,a} e_2 &= c(\alpha)\langle e_2, a\rangle a
        + (\cos\alpha)e_2 + (\sin\alpha)a \times e_2\\
        &= c(\alpha)a_2 (a_1e_1 + a_2e_2 + a_3e_3) + (\cos\alpha)e_2\\
        &\quad + (\sin\alpha)(a_1e_1 + a_2e_2 + a_3e_3) \times e_2\\
        &= (-a_3\sin\alpha + a_1a_2c(\alpha))e_1
        + (\cos\alpha + a_2^2c(\alpha)) e_2\\
        &\quad + (a_1\sin\alpha + a_2a_3c(\alpha))e_3
    \end{align*}
    \begin{align*}
        R_{\alpha,a} e_3 &= c(\alpha)\langle e_3, a\rangle a
        + (\cos\alpha)e_3 + (\sin\alpha)a \times e_3\\
        &= c(\alpha)a_3 (a_1e_1 + a_2e_2 + a_3e_3) + (\cos\alpha)e_3\\
        &\quad + (\sin\alpha)(a_1e_1 + a_2e_2 + a_3e_3) \times e_3\\
        &= (a_2\sin\alpha + a_1a_3c(\alpha))e_1
        + (-a_1\sin\alpha + a_2a_3c(\alpha)) e_2\\
        &\quad + (\cos\alpha + a_3^2c(\alpha))e_3
    \end{align*}
    Then $R_{\alpha, a}$ with respect to the standard basis for $\R^3$ is
    given by
    \begin{align*}
        R_{\alpha, a} = \begin{pmatrix}
            \cos\alpha + a_1^2c(\alpha)
            & -a_3\sin\alpha + a_1a_2c(\alpha)
            & a_2\sin\alpha + a_1a_3c(\alpha) \\
            a_3\sin\alpha + a_1a_2c(\alpha)
            & \cos\alpha + a_2^2c(\alpha)
            & -a_1\sin\alpha + a_2a_3c(\alpha) \\
            -a_2\sin\alpha + a_1a_3c(\alpha)
            & a_1\sin\alpha + a_2a_3c(\alpha)
            & \cos\alpha + a_3^2c(\alpha)
        \end{pmatrix}
    \end{align*}
    Suppose now that $R_{\alpha, a} = R_{\beta, b}$ then must be that
    $\tr(R_{\alpha, a}) = \tr(R_{\beta, b})$ hence
    \begin{align*}
        3\cos\alpha + a_1^2c(\alpha) + a_2^2c(\alpha) + a_3^2c(\alpha)
        = 3\cos\beta + b_1^2c(\beta) + b_2^2c(\beta) + b_3^2c(\beta)\\
        3(\cos\alpha - \cos\beta) + a_1^2c(\alpha) - b_1^2c(\beta)
        + a_2^2c(\alpha) - b_2^2c(\beta) + a_3^2c(\alpha) - b_3^2c(\beta)
        =  0
    \end{align*}
    If $\alpha = \beta = 0$ with $a,b$ arbitrary the equation holds.

    If $0 < \alpha = \beta < \pi$ and $a = b$ then the equation also holds.

    Finally if $\alpha = \beta = \pi$ we have that
    \begin{align*}
        a_1^2c(\pi) - b_1^2c(\pi) + a_2^2c(\pi) - b_2^2c(\pi)
        + a_3^2c(\pi) - b_3^2c(\pi) &= 0\\
        2a_1^2 - 2b_1^2 + 2a_2^2 - 2b_2^2 + 2a_3^2 - 2b_3^2 &= 0\\
        a_1^2 - b_1^2 + a_2^2 - b_2^2 + a_3^2 - b_3^2 &= 0
    \end{align*}
    So the only way for this equation to hold is that $a = \pm b$.

    \item [(iv)] We know that $\tr(R_{\alpha, a}) = 1 + 2\cos\alpha$ hence
    \begin{align*}
        2\cos\alpha &= \tr(R_{\alpha, a}) - 1\\
        \cos\alpha &= \frac{1}{2}(\tr(R_{\alpha, a}) - 1)
    \end{align*}
    Also, $R_{\alpha, a}^t$ is
    \begin{align*}
        R_{\alpha, a}^t = \begin{pmatrix}
            \cos\alpha + a_1^2c(\alpha)
            & a_3\sin\alpha + a_1a_2c(\alpha)
            & -a_2\sin\alpha + a_1a_3c(\alpha)\\
            %
            -a_3\sin\alpha + a_1a_2c(\alpha)
            & \cos\alpha + a_2^2c(\alpha)
            & a_1\sin\alpha + a_2a_3c(\alpha)\\
            %
            a_2\sin\alpha + a_1a_3c(\alpha)
            & -a_1\sin\alpha + a_2a_3c(\alpha)
            & \cos\alpha + a_3^2c(\alpha)
        \end{pmatrix}
    \end{align*}
    Then 
    \begin{align*}
        &R_{\alpha, a} - R_{\alpha, a}^t = \\
        &\begin{pmatrix}
            \cos\alpha + a_1^2c(\alpha)
            & -a_3\sin\alpha + a_1a_2c(\alpha)
            & a_2\sin\alpha + a_1a_3c(\alpha) \\
            a_3\sin\alpha + a_1a_2c(\alpha)
            & \cos\alpha + a_2^2c(\alpha)
            & -a_1\sin\alpha + a_2a_3c(\alpha) \\
            -a_2\sin\alpha + a_1a_3c(\alpha)
            & a_1\sin\alpha + a_2a_3c(\alpha)
            & \cos\alpha + a_3^2c(\alpha)
        \end{pmatrix}\\
        &- \begin{pmatrix}
            \cos\alpha + a_1^2c(\alpha)
            & a_3\sin\alpha + a_1a_2c(\alpha)
            & -a_2\sin\alpha + a_1a_3c(\alpha)\\
            %
            -a_3\sin\alpha + a_1a_2c(\alpha)
            & \cos\alpha + a_2^2c(\alpha)
            & a_1\sin\alpha + a_2a_3c(\alpha)\\
            %
            a_2\sin\alpha + a_1a_3c(\alpha)
            & -a_1\sin\alpha + a_2a_3c(\alpha)
            & \cos\alpha + a_3^2c(\alpha)
        \end{pmatrix}\\
        &= \begin{pmatrix}
            0
            & -2a_3\sin\alpha
            & 2a_2\sin\alpha \\
            2a_3\sin\alpha
            & 0
            & -2a_1\sin\alpha \\
            -2a_2\sin\alpha
            & 2a_1\sin\alpha
            & 0
        \end{pmatrix}\\
        &= 2\sin\alpha\begin{pmatrix}
            0 & -a_3 & a_2 \\
            a_3 & 0 & -a_1 \\
            -a_2 & a_1 & 0
        \end{pmatrix}
    \end{align*}
    On the other hand, suppose there is an angle $\beta \in \R$ where
    $0 < \beta < \pi$ and an axis of rotation $b \in \R^3$ such that
    $R_{\alpha, a} = R_{\beta, b}$.
    Since the trace does not depend on the representation of
    $R_{\alpha, a}$ must be that
    \begin{align*}
        \cos\alpha = \frac{1}{2}(\tr(R_{\alpha, a}) - 1)
        = \frac{1}{2}(\tr(R_{\beta, b}) - 1) = \cos\beta
    \end{align*} 
    This implies that $\alpha = \beta$.
    Also, we see that $r_a = (R_{\alpha, a} - R_{\alpha, a}^t)/2\sin\alpha$
    and since $0<\alpha < \pi$ then $\sin\alpha \neq 0$ so
    \begin{align*}
        r_a = \frac{(R_{\alpha, a} - R_{\alpha, a}^t)}{2\sin\alpha}
        = \frac{(R_{\beta, b} - R_{\beta, b}^t)}{2\sin\beta} = r_b
    \end{align*}
    Which implies that $a = b$ since the entries of $r_a$ and $r_b$ are the 
    components of $a$ and $b$ respectively.
    
    Let now, $\alpha = 0$ then
    $$R_{0, a} = \begin{pmatrix}
        1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1
    \end{pmatrix}$$
    for any $a \in \R^3$ so in this case is not uniquely determined by
    $R_{0, a}$.

    Finally, let $\alpha = \beta =\pi$ then as mentioned if we write $R_{\pi, a}$ as
    $(r_{ij})$ and $R_{\pi, b}$ as $(s_{ij})$ then $(r_{ij}) = (s_{ij})$.
    Also, we know that $2a_i^2 = 1 + r_{ii}$ and that $2a_ia_j = r_{ij}$
    then must be that
    \begin{align*}
        2a_i^2 = 1 + r_{ii} = 1 + s_{ii} = 2b_i^2  
    \end{align*}
    Which implies that $a_i = \pm b_i$. Also, must be that
    \begin{align*}
        a_ia_j = r_{ij} = s_{ij} = b_ib_j
    \end{align*}
    Since at least one $a_i\neq 0$ let $a_1 \neq 0$ and suppose $a_1 = b_1$
    then from the second equation we get that $a_j = b_j$.
    But if we take $a_1 = -b_1$ then we get that $a_j = -b_j$.

    Therefore $R_{\pi,a}$ determines $a$ to within a factor of $\pm 1$.
\end{itemize}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 1.6.13}}
    Let us select our basis such that $v \in \R^n$ is a linear combination of
    $e_1$ and $e_2$ then we can write that 
    $v = (\cos\theta, \sin\theta, 0, ..., 0)$.\\
    On the other hand, let us we define $R(t) \in \bm{SO}(n)$ as follows
    \begin{align*}
        R(t) = \begin{pmatrix}
            \cos(\theta t) & \sin(\theta t) & 0 & ... & 0\\
            -\sin(\theta t) & \cos(\theta t) & 0 & ... & 0\\
            0 & 0 & 1 & ... & 0\\
            \vdots & \vdots & \vdots & ... & \vdots\\
            0 & 0 & 0 & ... & 1\\
        \end{pmatrix}
    \end{align*}
    Then we see that $R(0) = I_n$ but also we have that
    \begin{align*}
        R(1)v = \begin{pmatrix}
            \cos^2\theta + \sin^2\theta\\
            -\sin\theta\cos\theta + \cos\theta\sin\theta\\
            0 \\ \vdots\\ 0
        \end{pmatrix}
        = \begin{pmatrix}
            1\\ 0\\ 0 \\ \vdots\\ 0
        \end{pmatrix} = e_1
    \end{align*}
    Let $R \in \bm{SO}(n)$ then taking $R(t)R$ as our path map we see that
    $$R(0)R = I_n R = R$$
    Also, the first column of $R$ is given by $R e_1$ let us name it $r_1$ then
    by what we have proven we see that
    $$R(1)Re_1 = R(1)r_1 = e_1$$
    So $R(t)R$ connects $R$ to a block-diagonal matrix of the form
    \begin{align*}
        \begin{pmatrix}
            1 & \\
            & R_1 
        \end{pmatrix}
    \end{align*}
    Where $R_1 \in \bm{SO}(n - 1)$ since $R(t)$ and $R$ are in $\bm{SO}(n)$.\\
    Therefore by induction we can connect any matrix $R \in \bm{SO}(n)$, and 
    hence $\bm{SO}(n)$ is connected.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 1.6.14}}
Let $R \in \bm{SO}(3)$ then with respect to the basis $(a, b, c)$ we can
write $R$ as 
\begin{align*}
    R = \begin{pmatrix}
        1 & 0 & 0\\
        0 & \cos\theta & -\sin\theta\\
        0 & \sin\theta & \cos\theta\\
    \end{pmatrix}
\end{align*}
Let $v$ be a vector in the $a$ direction then $v$ can be written as
\begin{align*}
    v = \begin{pmatrix}\|v\| \\ 0 \\ 0\end{pmatrix}
\end{align*}
Hence
\begin{align*}
    Rv = \begin{pmatrix}
        1 & 0 & 0\\
        0 & \cos\theta & -\sin\theta\\
        0 & \sin\theta & \cos\theta\\
    \end{pmatrix} \cdot \begin{pmatrix}\|v\| \\ 0 \\ 0\end{pmatrix}
    = \begin{pmatrix}
        \|v\| \\ 0 \\ 0 
    \end{pmatrix} = v
\end{align*}
So we can write that $Rv - v = 0$ or $(R - I)v = 0$. This implies that $v$
is an eigenvector of $R$ with eigenvalue $1$.

Now let $w \in N_v$ where $N_v$ is the plane orthogonal to $v$ this implies
that $\inprd{w,v} = 0$ then we see that
\begin{align*}
    \inprd{Rw, v} = \inprd{Rw, Rv} = \inprd{w, v} = 0
\end{align*}
So $R$ takes any element of the plane orthogonal to $v$ into itself.

Finally, let $w$ be in the direction of $b$ (orthogonal to $v$) so
\begin{align*}
    w = \begin{pmatrix} 0 \\ \|w\| \\ 0 \end{pmatrix}
\end{align*} 
Then
\begin{align*}
    Rw = \begin{pmatrix}
        1 & 0 & 0\\
        0 & \cos\theta & -\sin\theta\\
        0 & \sin\theta & \cos\theta\\
    \end{pmatrix}\cdot \begin{pmatrix} 0 \\ \|w\| \\ 0 \end{pmatrix}
    = \begin{pmatrix}
        0 \\ \|w\|\cos\theta \\ \|w\|\sin\theta
    \end{pmatrix}
    = \|w\|(\cos\theta b + \sin\theta c)
\end{align*} 
Therefore we get an equation showing the rotation of $w$ by an angle of
$\theta$ and hence applying $R$ we get a rotation by some angle $\theta$
around the axis $v$.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{Hall: 1.6.15}}
Let $R \in \bm{SO}(n)$
\begin{itemize}
    \item [(a)] Let $v \in \C^n$ be an eigenvector of $R$ with eigenvalue
    $\lambda \in \C$ and suppose $\lambda$ is not real.
    Let $V \subset \R^n$ be the two-dimensional span of
    $u = (v + \overline{v})/2$ and $w = (v - \overline{v})/(2i)$.

    Let us compute $R(au + bw)$ where $a,b \in \R$ as follows
    \begin{align*}
        R(au + bw) &= R\bigg(a\frac{v + \overline{v}}{2} + b\frac{v - \overline{v}}{2i}\bigg)\\
        &= a\frac{Rv + \overline{Rv}}{2} + b\frac{Rv - \overline{Rv}}{2i}\\
        &= a\frac{\lambda v + \overline{\lambda v}}{2}
        + b\frac{\lambda v - \overline{\lambda v}}{2i}
    \end{align*}
    Let us write $\lambda = \alpha + i\beta$, $\overline{\lambda} = \alpha - i\beta$,
    $v = u + iw$ and $\overline{v} = u - iw$ then
    \begin{align*}
        R(au + bw)
        &= a\frac{(\alpha + i\beta)(u + iw) + (\alpha - i\beta)(u - iw)}{2} + \\
        &\quad+ b\frac{(\alpha + i\beta)(u + iw) - (\alpha - i\beta)(u - iw)}{2i}\\
        &= \frac{a}{2}
        [\alpha(u + iw) + i\beta(u + iw) + \alpha(u - iw) - i\beta(u - iw)] - \\
        &\quad- \frac{ib}{2}[
            \alpha(u + iw) + i\beta(u + iw) - \alpha(u - iw) + i\beta(u - iw)
        ]\\
        &= \frac{a}{2}
        [\alpha u + \alpha iw + i\beta u - \beta w
        + \alpha u - \alpha iw - i\beta u - \beta w] - \\
        &\quad- \frac{ib}{2}
        [\alpha u + \alpha iw + i\beta u - \beta w 
        - \alpha u + \alpha iw + i\beta u + \beta w]\\
        &= \frac{a}{2}[2\alpha u - 2\beta w]
        - \frac{ib}{2}[2i\alpha w + 2i\beta u]\\
        &= a\alpha u - a\beta w + b\alpha w + b \beta u\\
        &= (a\alpha + b\beta)u + (b\alpha-a\beta)w
    \end{align*}
    Therefore $R(au + bw)$ is a linear combination of $u$ and $w$ and hence
    $R(au + bw) \in V$.

    Finally, let the restriction of $R$ to $V$  be $R'$ then
    $R' \in \bm{SO}(2)$ since $V$ is two-dimensional. So $R'$ by definition
    has determinant 1.

\cleardoublepage
    \item [(b)] Let $V\subset \R^n$ be invariant under both $R$ and $R^{-1}$
    so if $v \in V$ then $Rv \in V$ and $R^{-1}v \in V$.
    We know that
    \begin{align*}
        V^\perp = \{w \in \R^n :\inprd{w,v} = 0 \text{ for all }v \in V\}
    \end{align*}
    We want to prove that $V^\perp$ is also invariant under $R$ and $R^{-1}$.

    Let $w \in V^\perp$ we want to prove that $Rw \in V^\perp$. Let also
    $u\in V$ which can be written as $u = Rv$ for some $v \in V$ then
    \begin{align*}
        \inprd{u, Rw} = \inprd{Rv, Rw} = \inprd{v, w} = 0
    \end{align*}
    Where we used the property determined in Duistermaat 2.4(ii). Hence 
    $Rw \in V^\perp$ so $R$ is invariant under $V^\perp$.

    Finally, we want to prove that $R^{-1}w \in V^\perp$ but first let us note
    that $R^{-1} = R^t$ then if $u = R^tv \in V$ then
    \begin{align*}
        \inprd{u, R^tw} = \inprd{R^tv, R^tw} = \inprd{v, w} = 0
    \end{align*}
    Where we used the property determined in Duistermaat 2.4(iii). Hence 
    $R^tw \in V^\perp$ so $R^t = R^{-1}$ is invariant under $V^\perp$.

\cleardoublepage
    \item [(c)] By part (a) and (b) we know that if $R \in \bm{SO}(n)$ with an
    eigenvalue $\lambda \in \C^n$ and $\lambda$ is not real then we can form
    $V$ and $W = V^\perp$ which are invariant under $R$.

    Let $n = 2k$ we want to prove by induction on $k$ that $R$ has the required
    form.
    
    For the base case, suppose we take $R \in \bm{SO}(2)$ then we know that
    $R$ has the form
    \begin{align*}
        R = \begin{pmatrix}
            \cos\theta_1 & -\sin\theta_1\\
            \sin\theta_1 & \cos\theta_1
        \end{pmatrix}
    \end{align*}
    So in a 2-dimensional space $R$ has the effect of a planar rotation.

    Now, by the induction hypotesis, $R\in \bm{SO}(2(k - 1))$ has
    the form
    \begin{align*}
        R = \begin{pmatrix}
            \cos\theta_1 & -\sin\theta_1 &        &                  & \\
            \sin\theta_1 & \cos\theta_1  &        &                  & \\
                         &               & \ddots &                  & \\
                         &               &        & \cos\theta_{k-1} & -\sin\theta_{k-1}\\
                         &               &        & \sin\theta_{k-1} & \cos\theta_{k-1}
        \end{pmatrix}
    \end{align*}
    With respect to a basis
    $\{v_1, v_2, w_1, ..., w_{2k -4}\} \subset V_{2(k-1)} \cup W_{2(k-1)}$
    where $V_{2(k-1)}$ is the 2-dimensional space spanned as in part (a) in
    $\bm{SO}(2(k-1))$ and $W_{2(k-1)} = V^\perp_{2(k-1)}$.
    
    Then in $\bm{SO}(2k)$ must be that $W_{2k} = V_{2(k-1)} \cup W_{2(k-1)}$
    and hence $V_{2k}$ is orthogonal to $W_{2k}$, so $R$ must act as a planar
    rotation with respect to $V_{2k}$ hence we must add a rotation block to
    $R$ and therefore $R$ must be of the form 
    \begin{align*}
        R = \begin{pmatrix}
            \cos\theta_1 & -\sin\theta_1 &        &                  & \\
            \sin\theta_1 & \cos\theta_1  &        &                  & \\
                         &               & \ddots &                  & \\
                         &               &        & \cos\theta_{k} & -\sin\theta_{k}\\
                         &               &        & \sin\theta_{k} & \cos\theta_{k}
        \end{pmatrix}
    \end{align*}

    Let now $n = 2k +1$ then for the base case, we take $R \in \bm{SO}(3)$ that
    we know $R$ has the form
    \begin{align*}
        R = \begin{pmatrix}
            \cos\theta_1 & -\sin\theta_1 & 0\\
            \sin\theta_1 & \cos\theta_1 & 0\\
            0 & 0 & 1
        \end{pmatrix}
    \end{align*}
    Again $R$ in a 3-dimensional space has the effect of a planar rotation
    leaving one the components unchanged.

    So in the same way, in $\bm{SO}(2k)$ must be that
    $W_{2k} = V_{2(k-1)} \cup W_{2(k-1)}$ where $V_{2k}$ is orthogonal
    to $W_{2k}$, so $R$ must act as a planar rotation with respect to $V_{2k}$
    hence we must add a rotation block to $R$ and therefore $R$ must be of the
    form 
    \begin{align*}
        R = \begin{pmatrix}
            \cos\theta_1 & -\sin\theta_1 &        &                & & \\
            \sin\theta_1 & \cos\theta_1  &        &                & & \\
                         &               & \ddots &                & & \\
                         &               &        & \cos\theta_{k} & -\sin\theta_{k} & \\
                         &               &        & \sin\theta_{k} & \cos\theta_{k} & \\
                         &               &        &                & & 1
        \end{pmatrix}
    \end{align*}

    Now, suppose $\lambda \in \R^n$ then $\lambda = 1$ so we see that
    \begin{align*}
        R\bigg(a\frac{v + \overline{v}}{2} + b\frac{v - \overline{v}}{2i}\bigg)
        &= a\frac{Rv + \overline{Rv}}{2} + b\frac{Rv - \overline{Rv}}{2i}\\
        &= a\frac{\lambda v + \overline{\lambda v}}{2}
        + b\frac{\lambda v - \overline{\lambda v}}{2i}\\
        &= a\frac{v + \overline{v}}{2}
        + b\frac{v - \overline{v}}{2i}
    \end{align*}
    So $V$ is also invariant under $R$. Then for $n = 2k +1$ the same proof
    given above applies.

    In the case $n = 2k$ and $\lambda= 1$ we see that in the base case must be
    that $\theta_1 = 0$ so $R$ is of the form
    \begin{align*}
        R = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}
    \end{align*}
    and hence it has an eigenvalue of $1$.
    But the same proof given above (for the case $n=2k$) still applies and
    hence $R$ is of the form 
    \begin{align*}
        R = \begin{pmatrix}
            1 & 0  &        &        & \\
            0 & 1  &        &        & \\
                   &        & \ddots &                & \\
                   &        &        & \cos\theta_{k} & -\sin\theta_{k} \\
                   &        &        & \sin\theta_{k} & \cos\theta_{k}
        \end{pmatrix}
    \end{align*}
\end{itemize}
\end{proof}



\end{document}
